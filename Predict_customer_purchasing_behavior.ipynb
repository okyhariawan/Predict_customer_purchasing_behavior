{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Main Objective of the Analysis"
      ],
      "metadata": {
        "id": "ImX9Iew5OCH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary objective of this analysis is to predict customer purchasing behavior based on features such as product preferences, transaction amounts, and visit frequency. The goal is to develop a model that accurately predicts sales outcomes and provides insights into key factors driving sales in the coffee shop, ultimately assisting in optimizing marketing strategies and inventory management."
      ],
      "metadata": {
        "id": "C2hLM3evOlU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brief Description of the Dataset"
      ],
      "metadata": {
        "id": "YdUpc7z0OrjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Coffee Shop Sales.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows and summary information\n",
        "data.info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtUPz754NWvQ",
        "outputId": "948e8a6f-f88f-4045-d75b-481fb437e16b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 149116 entries, 0 to 149115\n",
            "Data columns (total 11 columns):\n",
            " #   Column            Non-Null Count   Dtype         \n",
            "---  ------            --------------   -----         \n",
            " 0   transaction_id    149116 non-null  int64         \n",
            " 1   transaction_date  149116 non-null  datetime64[ns]\n",
            " 2   transaction_time  149116 non-null  object        \n",
            " 3   transaction_qty   149116 non-null  int64         \n",
            " 4   store_id          149116 non-null  int64         \n",
            " 5   store_location    149116 non-null  object        \n",
            " 6   product_id        149116 non-null  int64         \n",
            " 7   unit_price        149116 non-null  float64       \n",
            " 8   product_category  149116 non-null  object        \n",
            " 9   product_type      149116 non-null  object        \n",
            " 10  product_detail    149116 non-null  object        \n",
            "dtypes: datetime64[ns](1), float64(1), int64(4), object(5)\n",
            "memory usage: 12.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "he dataset contains 149,116 transactions from a coffee shop. The key attributes in the dataset are:\n",
        "\n",
        "- transaction_id: Unique identifier for each transaction.\n",
        "- transaction_date: Date when the transaction took place.\n",
        "- transaction_time: Time when the transaction occurred.\n",
        "- transaction_qty: Quantity of items purchased in the transaction.\n",
        "- store_id: Identifier for the store where the transaction occurred.\n",
        "- store_location: The location of the store.\n",
        "- product_id: Identifier for the product sold.\n",
        "- unit_price: The price per unit of the product.\n",
        "- product_category: Category of the product (e.g., Coffee, Tea).\n",
        "- product_type: Specific type within the product category (e.g., Gourmet brewed coffee).\n",
        "- product_detail: Detailed description of the product.\n",
        "\n",
        "The goal is to use these features to predict the sales outcome, which could be represented by the transaction_qty, or to identify patterns in purchasing behavior."
      ],
      "metadata": {
        "id": "Y4LyYC6_PNdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration and Cleaning"
      ],
      "metadata": {
        "id": "aE0oIjYoPbVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Exploring the distribution of key features\n",
        "transaction_qty_distribution = data['transaction_qty'].describe()\n",
        "unit_price_distribution = data['unit_price'].describe()\n",
        "\n",
        "missing_values, transaction_qty_distribution, unit_price_distribution\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXyUqo5rPcIV",
        "outputId": "5a0169f2-3f5e-4f91-dc5e-f29fded046f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(transaction_id      0\n",
              " transaction_date    0\n",
              " transaction_time    0\n",
              " transaction_qty     0\n",
              " store_id            0\n",
              " store_location      0\n",
              " product_id          0\n",
              " unit_price          0\n",
              " product_category    0\n",
              " product_type        0\n",
              " product_detail      0\n",
              " dtype: int64,\n",
              " count    149116.000000\n",
              " mean          1.438276\n",
              " std           0.542509\n",
              " min           1.000000\n",
              " 25%           1.000000\n",
              " 50%           1.000000\n",
              " 75%           2.000000\n",
              " max           8.000000\n",
              " Name: transaction_qty, dtype: float64,\n",
              " count    149116.000000\n",
              " mean          3.382219\n",
              " std           2.658723\n",
              " min           0.800000\n",
              " 25%           2.500000\n",
              " 50%           3.000000\n",
              " 75%           3.750000\n",
              " max          45.000000\n",
              " Name: unit_price, dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Missing Values: There are no missing values in the dataset, so no imputation is necessary.\n",
        "- Transaction Quantity (transaction_qty): The quantity of items purchased in transactions ranges from 1 to 8, with a mean of approximately 1.44. Most transactions involve purchasing 1 or 2 items.\n",
        "- Unit Price (unit_price): The price per unit varies significantly, ranging from $0.80 to $45.00, with a mean price of about $3.38.\n",
        "\n",
        "Since the data is clean, the next step is feature engineering and model training."
      ],
      "metadata": {
        "id": "B92O_mBxPf6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "VPpY-DdGPnO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train and compare three different classification models. For this example, let's assume we're predicting whether a transaction involves purchasing more than one item (transaction_qty > 1).\n",
        "\n",
        "We'll create a binary target variable large_order (1 if transaction_qty > 1, 0 otherwise) and use the following models:\n",
        "\n",
        "- Logistic Regression: As a baseline model.\n",
        "- Random Forest Classifier: To capture non-linear relationships and interactions between features.\n",
        "- Gradient Boosting Classifier: To optimize prediction performance by combining multiple weak learners."
      ],
      "metadata": {
        "id": "uPo7GxqCPrvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Feature engineering: create a binary target variable\n",
        "data['large_order'] = (data['transaction_qty'] > 1).astype(int)\n",
        "\n",
        "# Selecting features for the model\n",
        "features = ['store_id', 'unit_price', 'product_id']\n",
        "X = data[features]\n",
        "y = data['large_order']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Training the models\n",
        "# Logistic Regression\n",
        "logreg = LogisticRegression(random_state=42)\n",
        "logreg.fit(X_train_scaled, y_train)\n",
        "y_pred_logreg = logreg.predict(X_test_scaled)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Gradient Boosting Classifier\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "\n",
        "# Generating classification reports\n",
        "logreg_report = classification_report(y_test, y_pred_logreg)\n",
        "rf_report = classification_report(y_test, y_pred_rf)\n",
        "gb_report = classification_report(y_test, y_pred_gb)\n",
        "\n",
        "logreg_report, rf_report, gb_report\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pYHCMegPU3O",
        "outputId": "3c343bb4-bb88-42fc-8b38-2da04d513611"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('              precision    recall  f1-score   support\\n\\n           0       0.61      0.79      0.69     26247\\n           1       0.50      0.30      0.37     18488\\n\\n    accuracy                           0.59     44735\\n   macro avg       0.56      0.54      0.53     44735\\nweighted avg       0.57      0.59      0.56     44735\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.68      0.63      0.65     26247\\n           1       0.52      0.57      0.54     18488\\n\\n    accuracy                           0.60     44735\\n   macro avg       0.60      0.60      0.60     44735\\nweighted avg       0.61      0.60      0.61     44735\\n',\n",
              " '              precision    recall  f1-score   support\\n\\n           0       0.68      0.60      0.64     26247\\n           1       0.52      0.61      0.56     18488\\n\\n    accuracy                           0.60     44735\\n   macro avg       0.60      0.60      0.60     44735\\nweighted avg       0.62      0.60      0.61     44735\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three different models were trained and evaluated on the binary classification task of predicting whether a transaction involves more than one item:\n",
        "\n",
        "Logistic Regression:\n",
        "- Precision: 0.50 to 0.61\n",
        "- Recall: 0.30 to 0.79\n",
        "- F1-Score: 0.37 to 0.69\n",
        "- Accuracy: 0.59\n",
        "\n",
        "Random Forest Classifier:\n",
        "- Precision: 0.52 to 0.68\n",
        "- Recall: 0.57 to 0.63\n",
        "- F1-Score: 0.54 to 0.65\n",
        "- Accuracy: 0.60\n",
        "\n",
        "Gradient Boosting Classifier:\n",
        "- Precision: 0.52 to 0.68\n",
        "- Recall: 0.61 to 0.60\n",
        "- F1-Score: 0.56 to 0.64\n",
        "- Accuracy: 0.60\n",
        "\n",
        "Recommendation: The Random Forest Classifier is recommended as the final model due to its balance between precision, recall, and F1-score across the classes. It provides better overall performance than Logistic Regression and similar performance to Gradient Boosting, but with the added advantage of being more interpretable."
      ],
      "metadata": {
        "id": "9vioaOdOP0Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Findings and Insights"
      ],
      "metadata": {
        "id": "fPpYalN6QDUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature Importance: In the Random Forest model, the unit_price and product_id were significant predictors of whether a customer would purchase more than one item.\n",
        "- Customer Behavior: The analysis suggests that higher unit prices and specific product categories may encourage bulk purchases."
      ],
      "metadata": {
        "id": "EN4EZIAYQFaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Flaws and Next Steps"
      ],
      "metadata": {
        "id": "nxp3XfsUQIL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Class Imbalance: The dataset shows a significant imbalance between single-item and multi-item purchases. Techniques such as oversampling or undersampling could be explored to improve model performance.\n",
        "- Feature Expansion: Additional features such as time of day, customer demographics, or loyalty program status could improve prediction accuracy.\n",
        "- Model Refinement: Experimenting with hyperparameter tuning and different ensemble methods (like XGBoost) could further optimize the model."
      ],
      "metadata": {
        "id": "ICxGLbhGQKh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "D-eotnbUQNCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis highlights the potential for predictive modeling to understand customer behavior in a coffee shop setting. By implementing the Random Forest model, the business can better anticipate customer needs and optimize inventory and marketing strategies."
      ],
      "metadata": {
        "id": "CuzFR3EVQO1L"
      }
    }
  ]
}